<!doctype html>
<!-- Based on impress.js Copyright 2011-2012 Bartek Szopka

Released under the MIT and GPL (version 2 or later) Licenses.

Note, this is a total mess of inconsistent css, html, javascript and content.
It's probably a good example of how difficult it is for me to separate content with presentation in practice.
-->

<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=1024" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<title> </title>

<meta name="description" content="" />
<meta name="author" content="" />

<link rel="stylesheet" type="text/css" href="css/reset.css" />

<link rel="stylesheet" type="text/css"
href="http://fonts.googleapis.com/css?family=Righteous|Comfortaa">

<link rel="stylesheet" href="lib/fontawesome/css/font-awesome.min.css">
<!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"> -->
<!-- basic setup -->
<!-- <link rel="stylesheet" type="text/css" href="css/base.css" /> -->
<!-- menus and buttons -->
<link rel="stylesheet" type="text/css" href="css/navmenu.css" />

<!-- presentation-specific styling, which might be done with less instead of css -->
<link rel="stylesheet" type="text/css" href="css/style.css" />
<!-- <link rel="stylesheet/less" type="text/css" href="less/style.less"> -->


<link rel="shortcut icon" href="img/favicon.png" />
<link rel="apple-touch-icon" href="img/apple-touch-icon.png" />
</head>

<body class="impress-not-supported">

<!--
    For example this fallback message is only visible when there is `impress-not-supported` class on body.
-->
<div class="fallback-message">
    <p>Your browser <b>doesn't support the features required</b> by impress.js, so you are presented with a simplified version of this presentation.</p>
    <p>For the best experience please use the latest <b>Chrome</b>, <b>Safari</b> or <b>Firefox</b> browser.</p>
</div>

<div id="impress">

    <!-- each slide must be a div with class step,  must have some co-ordinates -->
    <div id="title" class="step" data-x="0" data-y="0" data-z="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1">
        <h1>Word Embeddings</h1>
        <h2>A Journey to Low-Dimensional Word Vector Space</h2>
        <p>
        <a href="mailto:Alexander.OConnor@dcu.ie">
            Alexander.OConnor@dcu.ie
        </a>
        </p>
        <p>
        <i class="fa fa-twitter fa-large"></i>@
        <a href="https://www.twitter.com/uberalex">uberalex</a>
        </p>
        <div id="logofoot">
            <a href="http://www.dcu.ie"><img src="img/dcu.jpg" /></a>
            <a href="http://www.adaptcentre.ie"><img src="img/adapt.png" /></a>
        </div>
        <div class="notes">
            <!--speaker notes-->
            <p>You can see my notes and details here. Please do get in touch if
            you have any comments or questions.</p>
        </div>
    </div>
    <!-- Language and Defining Embeddings -->
    <div id="Language" class="step" data-x="1200" data-y="0" data-z="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1">
        <p>The quick, brown fox jumped over the lazy dog.</p>
        <div class="notes">
            <!--speaker notes-->
            <p>Before we start some very simple basics about language as data.
            Normally when we think about language data, it's a corpus of
            documents. Each document has paragraphs, which are groups of
            sentences. The sentences themselves are made up of words, the
            punctuation between the words.</p>
        </div>
    </div>
    <div id="Zipf" class="step" data-x="1200" data-y="0" data-z="-1000"
        data-rotate-x="90" data-rotate-y="0" data-rotate-z="" data-scale="1">
        <!--speaker notes-->
        <div class="notes">Data follows the curve of a broken power law. The vast majority
            of the uses of words come from a very small group. For a variety of
            reasons, we use the same words repeatedly ('I', 'The', 'Is', etc)
            <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">https://en.wikipedia.org/wiki/Zipf%27s_law</a>
            <p class="small">A plot of the rank versus frequency for the first
            10 million words in 30 Wikipedias (dumps from October 2015) in a
            log-log scale.</p>
            </p>
        </div>
    </div>
    <div id="Distributional" class="step" data-x="2400" data-y="0" data-z="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1">
        <h1>The Distributional Hypothesis</h1>
        <h2>"a word is characterized by the company it keeps"</h2>
        <div id="Sentence">
                        <ol>
                            <li><p class="embed">Banks can create new money when they make a loan.</li>
                            <li><p class="embed">The boat struck the bank full
                            tilt.</p></li>
            </ol>
            </p>
        </div>
        <div class="notes">
            <!--speaker notes-->
            <p>We can only learn the meaning of words from their asssociation
            with other words.</p>
            <p class="small">Firth, J.R. (1957). A synopsis of linguistic theory 1930-1955. In
            Studies in Linguistic Analysis, pp. 1-32. Oxford: Philological
            Society. Reprinted in F.R. Palmer (ed.), Selected Papers of J.R.
            Firth 1952-1959, London: Longman (1968).<br />
            Harris, Z. (1954). Distributional structure. Word, 10(23): 146-162.</p>
        </div>
    </div>
    <div id="Embedding" class="step" data-x="2400" data-y="1200" data-z="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1">
        <p>A <strong>Word Embedding</strong> is a parameterised function that
        maps words from a vocabulary to lower-dimension vectors of real
        weights</p>
        <ul>
            <li><p class="embed">W('King') = [0.5, 0.7]</p></li>
            <li><p class="embed">W('Man') = [0.5, 0.2]</p></li>
        </ul>
        <div class="notes">
            <!--speaker notes-->
            <p></p>
        </div>
    </div>
<div id="Vectors" class="step" data-x="2400" data-y="2400" data-z="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1">
    <p class="small">Image Source: <a
        href="https://www.tensorflow.org/versions/0.6.0/tutorials/word2vec/index.html
        <">https://www.tensorflow.org/versions/0.6.0/tutorials/word2vec/index.html</a>
    <div class="notes">
        <!--speaker notes-->
        So we can begin to see that the distributional hypothesis tells us
        something more than just how words are used. It begins to tell us
        how underlying relationships between concepts might exist.
    </div>
</div>

    <!-- Embedding Words for Fun &amp; Profit -->
    <div id="FunAndProfit" class="step" data-x="3600" data-y="0" data-z="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1">
        <h1>Word2Vec (<a href="http://arxiv.org/abs/1310.4546">Mikolov et al.</a>)</small></h1>
    <p class="embed">The quick brown fox Jumped
    <strong>over</strong><sub>(V<sub>in</sub>)</sub> the lazy dog.</p><br /><br
        />
        <p>Maximise P(V<sub>out</sub>|V<sub>in</sub>) by learning the softmax
        probability via gradient descent. </p>
    <div class="notes">
        <!--speaker notes-->
        <p><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/">http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/</a></p>
        <p><a href="http://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994">http://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994</a></p>
    </div>
</div>
<div id="operations" class="step" data-x="4800" data-y="3600" data-z="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1">
    <h1>Operations on the embedding vector space</h1>
    <p class="embed">King + Woman - Man = Queen</p>
    <p class="embed">Obama + Russia - USA = Putin </p>
    <p class="embed">Breakfast, <del>Cereal</del>, Lunch,&nbsp;Dinner </p>
    <p class="embed">Bad is to Worse as Good is to Better</p>
    <p class="embed"></b>
<p>Use it to expand queries, find synonyms, disambiguate terms, group
items</p>
<div class="notes">
    Doesn't always work predictably, and sometimes produces complete
    nonsense. It's also often not bi-directional.
</div>

    </div>
    <div id="Translate" class="step" data-x="4800" data-y="4800" data-z="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1">
<img src="img/translate.png">
<ol>
    <li><p>Learn Monolingual Word Embeddings from lots of text</p></li>
    <li><p>Use a small bilingual dictionary to learn the linear projection</p></li>
    <li><p>Project unknown word from source embedding to target</p></li>
</ol>

<div class="notes">
    <p>Mikolov et. al, <a href="http://arxiv.org/abs/1309.4168">Exploiting Similarities among
        Languages for Machine Translation</a></p>

</div>

    </div>



    <!-- DIY -->
    <div id="DIY" class="step" data-x="3600" data-y="2400" data-z="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1">
<h2>Over to You</h2>
<p>Need lots of pre-processed text, so pre-trained models can be a good place to start.</p>
<p><a href="https://radimrehurek.com/gensim/">Gensim</a> (Python), <a
href="http://nlp.stanford.edu/projects/glove/">GloVe</a>,<a
        href="http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html">WEM
    </a>(R)</p>
    <p><a href="http://rare-technologies.com/making-sense-of-word2vec/">Making
        Sense of Word2Vec</a>,<a
        href="https://levyomer.wordpress.com/2014/04/25/linguistic-regularities-in-sparse-and-explicit-word-representations/">Levy
        &amp; Goldberg</a> 'Linguistic Regularities in Sparse and Explicit Word
    Representations.'</p>
    <div class="notes">
        <!--speaker notes-->
        <p>A lot of people use the google newsgroups data.</p>
    </div>
</div>
<!-- Conclusion -->
<div id="Final" class="step" data-x="6000" data-y="6000" data-z="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1">

</div>
<!-- keep an overview slide, you may wish to change the scaling and / or the co-ordinates -->

<div id="overview" class="step" data-x="1200" data-y="1000" data-z="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="2">
</div>

</div>
<!-- end of impress -->

<!-- a bar at the top with a timer. You could put other text or info in here by adding more spans -->
<div id="navbar"><div id="timebar">&nbsp;</div><p> <span id="timer">Click to Start Timer.</span></p><p><input id="notebox" type="checkbox" /><label for="notebox">show notes</label></p></div>

<!-- a little menu based on the slides -->
<div class="hint note">
<p>Use the spacebar or arrow keys to navigate</p>
</div>

<!-- clicking on this reveals the quick-navigation bar -->
<div id="menubutton" tabindex="1">
<i class="fa fa-bars fa-lg"></i>
</div>

<!-- quickly get to different slides here, the list is populated from the IDs of the slides -->
<div id="navmenu">
</div>



<script>
if ("ontouchstart" in document.documentElement) {
    document.querySelector(".hint").append('p').text('Tap on the left or right to navigate');
}
</script>

<!-- LESS CSS -->
<script src="lib/less.js/dist/less-1.7.5.js"></script>
<!-- Load the presentation tool -->
<script src="lib/impress.js/js/impress.js"></script>
<!-- Load D3 -->
<script src="lib/d3/d3.min.js"></script>
<!-- Load the timer -->
<script src="js/timer.js"></script>
<!-- Menu -->
<script src="js/navmenu.js"></script>
<!-- -->
<script src="js/diagram.js"></script>

<!-- Initialisation -->
<script>impress().init();</script>
<script>var time = 1000 * 60 * 10;
var clock = timer(time, update);
</script>
</body>
</html>
